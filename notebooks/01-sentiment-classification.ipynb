{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification of Movie Reviews\n",
    "\n",
    "This notebook builds a sentiment classifier to predict positive or negative sentiment from Rotten Tomatoes movie reviews using the Sentence Polarity dataset. The workflow covers data loading, text preprocessing, TF-IDF feature extraction, model training with hyperparameter tuning, and evaluation with K-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Reading in & Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/annie/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/annie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/annie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/annie/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np  # for numerical operations\n",
    "import pandas as pd  # for data manipulation\n",
    "import random  # for shuffling the data\n",
    "import nltk\n",
    "import re  # for handling regular expressions\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer  # for lemmatizing words\n",
    "from nltk.corpus import stopwords  # for stop word removal\n",
    "from nltk.tokenize import word_tokenize  # for tokenizing sentences into words\n",
    "nltk.download('punkt_tab')  # Downloads the 'punkt' tokenizer table used for tokenization of text into sentences or words\n",
    "\n",
    "# Downloading necessary NLTK resources\n",
    "nltk.download('stopwords')  # List of common stop words in English\n",
    "nltk.download('punkt')  # Pre-trained tokenizer models\n",
    "nltk.download('wordnet')  # WordNet lemmatizer dataset\n",
    "\n",
    "# Libraries for text feature extraction and model training\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Convert text into numerical features (TF-IDF)\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic regression for classification\n",
    "from sklearn.svm import LinearSVC  # Support Vector Machines for classification\n",
    "\n",
    "# Libraries for model evaluation\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix  # For model evaluation metrics\n",
    "from sklearn.model_selection import KFold, cross_val_score  # For cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Read Data Files\n",
    "\n",
    "The `Sentence Polarity` dataset contains 5,331 positive and 5,331 negative sentences. We'll load this dataset and prepare it for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0                  simplistic , silly and tedious . \n",
      "1  it's so laddish and juvenile , only teenage bo...\n",
      "2  exploitative and largely devoid of the depth o...\n",
      "3  [garbus] discards the potential for pathologic...\n",
      "4  a visually flashy but narratively opaque and e...\n"
     ]
    }
   ],
   "source": [
    "# Read the positive and negative sentiment files\n",
    "df_sent_pos = pd.read_csv('https://raw.githubusercontent.com/chrisvdweth/nus-cs4248x/refs/heads/master/1-foundations/data/corpora/sentence-polarity-dataset/sentence-polarity.neg', sep='\\t', header=None)  # Positive sentiment sentences\n",
    "df_sent_neg = pd.read_csv('https://raw.githubusercontent.com/chrisvdweth/nus-cs4248x/refs/heads/master/1-foundations/data/corpora/sentence-polarity-dataset/sentence-polarity.pos', sep='\\t', header=None)  # Negative sentiment sentences\n",
    "\n",
    "# Display the first few rows of the positive dataset to understand its structure\n",
    "print(df_sent_pos.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column to 'sentence' for clarity\n",
    "df_sent_pos.rename(columns={0: \"sentence\"}, inplace=True)\n",
    "df_sent_neg.rename(columns={0: \"sentence\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Data Preprocessing\n",
    "\n",
    "The sentences is preprocessed by defining a function called `preprocess_text` that performs the following:\n",
    "\n",
    "1. Converts text to lowercase.\n",
    "2. Removes punctuation using regular expressions.\n",
    "3. Removes extra whitespace.\n",
    "4. Tokenizes sentences into words.\n",
    "5. Removes stop words.\n",
    "6. Lemmatizes words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing function\n",
    "def preprocess_text(sentences):\n",
    "    # Convert all tokens to lowercase\n",
    "    sentences = [sentence.lower() for sentence in sentences]\n",
    "\n",
    "    # Remove punctuation using regex\n",
    "    sentences = [re.sub(r\"[^\\w\\s]\", \"\", sentence) for sentence in sentences]\n",
    "\n",
    "    # Remove extra whitespace between words\n",
    "    sentences = [\" \".join(sentence.split()) for sentence in sentences]\n",
    "\n",
    "    # Tokenize sentences into words\n",
    "    sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))  # Load English stop words\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        filtered_sentence = [word for word in sentence if word not in stop_words]\n",
    "        filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentences = []\n",
    "    for sentence in filtered_sentences:\n",
    "        lemmatized_sentence = [lemmatizer.lemmatize(word) for word in sentence]\n",
    "        lemmatized_sentences.append(lemmatized_sentence)\n",
    "\n",
    "    return [' '.join(sentence) for sentence in lemmatized_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Apply Preprocessing\n",
    "\n",
    "Preprocessing function is applied to both negative and positive sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rock destined 21st century new conan he going make splash even greater arnold schwarzenegger jeanclaud van damme steven segal\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the sentences\n",
    "pos_preprocessed_sentences = preprocess_text(df_sent_pos['sentence'])\n",
    "neg_preprocessed_sentences = preprocess_text(df_sent_neg['sentence'])\n",
    "\n",
    "# Print the first preprocessed negative sentence\n",
    "print(neg_preprocessed_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Combine Dataset\n",
    "\n",
    "The negative and positive sentences are merged into a single list called `sentences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine preprocessed positive and negative sentences\n",
    "sentences = pos_preprocessed_sentences + neg_preprocessed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Create Labels\n",
    "\n",
    "Labels (also targets) distinguish negative (labeled as `0`) and positive (labeled as `1`) sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list for all labels\n",
    "polarities = []\n",
    "polarities.extend([0] * len(df_sent_neg))  # Label negative sentences as 0\n",
    "polarities.extend([1] * len(df_sent_pos))  # Label positive sentences as 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Shuffle Data\n",
    "\n",
    "Randomly shuffle the dataset to ensure sentence are in random order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine sentences and labels into a single list\n",
    "combined = list(zip(sentences, polarities))\n",
    "\n",
    "# Shuffle the combined list\n",
    "random.shuffle(combined)\n",
    "\n",
    "# Split the shuffled list back into sentences and labels\n",
    "sentences[:], polarities[:] = zip(*combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Split Dataset\n",
    "\n",
    "The dataset is split into 80% for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 8529\n",
      "Size of test set: 2133\n"
     ]
    }
   ],
   "source": [
    "# Define train-test split ratio\n",
    "train_test_ratio = 0.8\n",
    "\n",
    "# Calculate the size of the training set\n",
    "train_set_size = int(train_test_ratio * len(sentences))\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test = sentences[:train_set_size], sentences[train_set_size:]\n",
    "y_train, y_test = polarities[:train_set_size], polarities[train_set_size:]\n",
    "\n",
    "# Print sizes of training and test sets\n",
    "print(\"Size of training set:\", len(X_train))\n",
    "print(\"Size of test set:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Vectorizing Texts, Training Models & Evaluating Their Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Transforming Text into Features\n",
    "\n",
    "To use text data in machine learning models, we need to convert it into a numerical format that algorithms can process. The TF-IDF (Term Frequency-Inverse Document Frequency) Vectorizer transforms our preprocessed text into numerical features by measuring how important each term is within a sentence while reducing the weight of commonly occurring words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Samples: 8529, #Features: 16471\n"
     ]
    }
   ],
   "source": [
    "# Import TF-IDF Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define the vectorizer with default parameters\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the training data into a TF-IDF matrix\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Check the number of samples and features\n",
    "num_samples, num_features = X_train_tfidf.shape\n",
    "print(\"#Samples: {}, #Features: {}\".format(num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in training set: 16501\n",
      "TF-IDF features: 16471\n",
      "Number of stopwords: 198\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic check - compare vocabulary size\n",
    "from collections import Counter\n",
    "all_words = ' '.join(X_train).split()\n",
    "unique_words = set(all_words)\n",
    "print(f\"Unique words in training set: {len(unique_words)}\")\n",
    "print(f\"TF-IDF features: {X_train_tfidf.shape[1]}\")\n",
    "print(f\"Number of stopwords: {len(stopwords.words('english'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train the Classifier\n",
    "\n",
    "Logistic Regression is a simple yet effective algorithm for binary classification tasks, such as predicting sentiment polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the Logistic Regression classifier\n",
    "logistic_regression_classifier = LogisticRegression().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluation the Classifier\n",
    "\n",
    "After training the model, we need to assess its performance on unseen test data. Evaluation involves transforming the test data into the same TF-IDF format as the training data, making predictions, and calculating key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.75      0.75      1060\n",
      "           1       0.76      0.76      0.76      1073\n",
      "\n",
      "    accuracy                           0.76      2133\n",
      "   macro avg       0.76      0.76      0.76      2133\n",
      "weighted avg       0.76      0.76      0.76      2133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform the test data into TF-IDF format\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Predict polarities for the test data\n",
    "y_pred = logistic_regression_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Import evaluation metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Generate and display the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model achieved an F1-score of 0.76 for sentiment classification, demonstrating significantly better performance than random guessing (~50%) with 76% precision and recall for both classes. While this result shows the model performs well above baseline, it indicates there is still room for improvement. The next steps involve exploring cross-validation and hyperparameter tuning techniques to further enhance the model's performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-movie-sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
