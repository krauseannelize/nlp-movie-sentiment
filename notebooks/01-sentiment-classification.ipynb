{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification of Movie Reviews\n",
    "\n",
    "This notebook builds a sentiment classifier to predict positive or negative sentiment from Rotten Tomatoes movie reviews using the Sentence Polarity dataset. The workflow covers data loading, text preprocessing, TF-IDF feature extraction, model training with hyperparameter tuning, and evaluation with K-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Reading in & Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/annie/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/annie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/annie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/annie/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np  # for numerical operations\n",
    "import pandas as pd  # for data manipulation\n",
    "import random  # for shuffling the data\n",
    "import nltk\n",
    "import re  # for handling regular expressions\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer  # for lemmatizing words\n",
    "from nltk.corpus import stopwords  # for stop word removal\n",
    "from nltk.tokenize import word_tokenize  # for tokenizing sentences into words\n",
    "nltk.download('punkt_tab')  # Downloads the 'punkt' tokenizer table used for tokenization of text into sentences or words\n",
    "\n",
    "# Downloading necessary NLTK resources\n",
    "nltk.download('stopwords')  # List of common stop words in English\n",
    "nltk.download('punkt')  # Pre-trained tokenizer models\n",
    "nltk.download('wordnet')  # WordNet lemmatizer dataset\n",
    "\n",
    "# Libraries for text feature extraction and model training\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Convert text into numerical features (TF-IDF)\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic regression for classification\n",
    "from sklearn.svm import LinearSVC  # Support Vector Machines for classification\n",
    "\n",
    "# Libraries for model evaluation\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix  # For model evaluation metrics\n",
    "from sklearn.model_selection import KFold, cross_val_score  # For cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Read Data Files\n",
    "\n",
    "The `Sentence Polarity` dataset contains 5,331 positive and 5,331 negative sentences. We'll load this dataset and prepare it for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  the rock is destined to be the 21st century's ...\n",
      "1  the gorgeously elaborate continuation of \" the...\n",
      "2                     effective but too-tepid biopic\n",
      "3  if you sometimes like to go to the movies to h...\n",
      "4  emerges as something rare , an issue movie tha...\n"
     ]
    }
   ],
   "source": [
    "# Read the positive and negative sentiment files\n",
    "df_sent_pos = pd.read_csv('../data/rt-polarity.pos', sep='\\t', header=None)  # Positive sentiment sentences\n",
    "df_sent_neg = pd.read_csv('../data/rt-polarity.neg', sep='\\t', header=None)  # Negative sentiment sentences\n",
    "\n",
    "# Alternatively, use GitHub raw links to read the datasets directly into pandas DataFrames\n",
    "# df_sent_pos = pd.read_csv('https://raw.githubusercontent.com/chrisvdweth/nus-cs4248x/refs/heads/master/1-foundations/data/corpora/sentence-polarity-dataset/sentence-polarity.neg', sep='\\t', header=None)  # Positive sentiment sentences\n",
    "# df_sent_neg = pd.read_csv('https://raw.githubusercontent.com/chrisvdweth/nus-cs4248x/refs/heads/master/1-foundations/data/corpora/sentence-polarity-dataset/sentence-polarity.pos', sep='\\t', header=None)  # Negative sentiment sentences\n",
    "\n",
    "# Display the first few rows of the positive dataset to understand its structure\n",
    "print(df_sent_pos.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column to 'sentence' for clarity\n",
    "df_sent_pos.rename(columns={0: \"sentence\"}, inplace=True)\n",
    "df_sent_neg.rename(columns={0: \"sentence\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Data Preprocessing\n",
    "\n",
    "The sentences is preprocessed by defining a function called `preprocess_text` that performs the following:\n",
    "\n",
    "1. Converts text to lowercase.\n",
    "2. Removes punctuation using regular expressions.\n",
    "3. Removes extra whitespace.\n",
    "4. Tokenizes sentences into words.\n",
    "5. Removes stop words.\n",
    "6. Lemmatizes words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing function\n",
    "def preprocess_text(sentences):\n",
    "    # Convert all tokens to lowercase\n",
    "    sentences = [sentence.lower() for sentence in sentences]\n",
    "\n",
    "    # Remove punctuation using regex\n",
    "    sentences = [re.sub(r\"[^\\w\\s]\", \"\", sentence) for sentence in sentences]\n",
    "\n",
    "    # Remove extra whitespace between words\n",
    "    sentences = [\" \".join(sentence.split()) for sentence in sentences]\n",
    "\n",
    "    # Tokenize sentences into words\n",
    "    sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))  # Load English stop words\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        filtered_sentence = [word for word in sentence if word not in stop_words]\n",
    "        filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentences = []\n",
    "    for sentence in filtered_sentences:\n",
    "        lemmatized_sentence = [lemmatizer.lemmatize(word) for word in sentence]\n",
    "        lemmatized_sentences.append(lemmatized_sentence)\n",
    "\n",
    "    return [' '.join(sentence) for sentence in lemmatized_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Apply Preprocessing\n",
    "\n",
    "Preprocessing function is applied to both negative and positive sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simplistic silly tedious\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the sentences\n",
    "pos_preprocessed_sentences = preprocess_text(df_sent_pos['sentence'])\n",
    "neg_preprocessed_sentences = preprocess_text(df_sent_neg['sentence'])\n",
    "\n",
    "# Print the first preprocessed negative sentence\n",
    "print(neg_preprocessed_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Combine Dataset\n",
    "\n",
    "The negative and positive sentences are merged into a single list called `sentences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine preprocessed positive and negative sentences\n",
    "sentences = pos_preprocessed_sentences + neg_preprocessed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Create Labels\n",
    "\n",
    "Labels (also targets) distinguish negative (labeled as `0`) and positive (labeled as `1`) sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list for all labels\n",
    "polarities = []\n",
    "polarities.extend([0] * len(df_sent_neg))  # Label negative sentences as 0\n",
    "polarities.extend([1] * len(df_sent_pos))  # Label positive sentences as 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Shuffle Data\n",
    "\n",
    "Randomly shuffle the dataset to ensure sentence are in random order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine sentences and labels into a single list\n",
    "combined = list(zip(sentences, polarities))\n",
    "\n",
    "# Shuffle the combined list\n",
    "random.shuffle(combined)\n",
    "\n",
    "# Split the shuffled list back into sentences and labels\n",
    "sentences[:], polarities[:] = zip(*combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Split Dataset\n",
    "\n",
    "The dataset is split into 80% for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 8529\n",
      "Size of test set: 2133\n"
     ]
    }
   ],
   "source": [
    "# Define train-test split ratio\n",
    "train_test_ratio = 0.8\n",
    "\n",
    "# Calculate the size of the training set\n",
    "train_set_size = int(train_test_ratio * len(sentences))\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test = sentences[:train_set_size], sentences[train_set_size:]\n",
    "y_train, y_test = polarities[:train_set_size], polarities[train_set_size:]\n",
    "\n",
    "# Print sizes of training and test sets\n",
    "print(\"Size of training set:\", len(X_train))\n",
    "print(\"Size of test set:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Vectorizing Texts, Training Models & Evaluating Their Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Transforming Text into Features\n",
    "\n",
    "To use text data in machine learning models, we need to convert it into a numerical format that algorithms can process. The TF-IDF (Term Frequency-Inverse Document Frequency) Vectorizer transforms our preprocessed text into numerical features by measuring how important each term is within a sentence while reducing the weight of commonly occurring words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Samples: 8529, #Features: 16513\n"
     ]
    }
   ],
   "source": [
    "# Import TF-IDF Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define the vectorizer with default parameters\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the training data into a TF-IDF matrix\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Check the number of samples and features\n",
    "num_samples, num_features = X_train_tfidf.shape\n",
    "print(\"#Samples: {}, #Features: {}\".format(num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in training set: 16543\n",
      "TF-IDF features: 16513\n",
      "Number of stopwords: 198\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic check - compare vocabulary size\n",
    "from collections import Counter\n",
    "all_words = ' '.join(X_train).split()\n",
    "unique_words = set(all_words)\n",
    "print(f\"Unique words in training set: {len(unique_words)}\")\n",
    "print(f\"TF-IDF features: {X_train_tfidf.shape[1]}\")\n",
    "print(f\"Number of stopwords: {len(stopwords.words('english'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train the Classifier\n",
    "\n",
    "Logistic Regression is a simple yet effective algorithm for binary classification tasks, such as predicting sentiment polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train the Logistic Regression classifier\n",
    "logistic_regression_classifier = LogisticRegression().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluation the Classifier\n",
    "\n",
    "After training the model, we need to assess its performance on unseen test data. Evaluation involves transforming the test data into the same TF-IDF format as the training data, making predictions, and calculating key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.75      0.75      1052\n",
      "           1       0.75      0.75      0.75      1081\n",
      "\n",
      "    accuracy                           0.75      2133\n",
      "   macro avg       0.75      0.75      0.75      2133\n",
      "weighted avg       0.75      0.75      0.75      2133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform the test data into TF-IDF format\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Predict polarities for the test data\n",
    "y_pred = logistic_regression_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Import evaluation metrics\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Generate and display the classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model achieved an F1-score of 0.75 for sentiment classification, demonstrating significantly better performance than random guessing (~50%) with 75% precision and recall for both classes. While this result shows the model performs well above baseline, it indicates there is still room for improvement. The next steps involve exploring cross-validation and hyperparameter tuning techniques to further enhance the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Training Using K-Fold Cross Validation\n",
    "\n",
    "In machine learning, a model's performance on a single train-test split can sometimes be misleading. A model might perform exceptionally well on one particular split by chance, while performing poorly on others. To build confidence in our model's generalization ability and ensure its robustness across different data subsets, we need a more rigorous evaluation approach. K-fold cross-validation is the industry-standard technique that addresses this challenge by systematically evaluating your model across multiple data splits, providing a comprehensive and reliable assessment of its real-world performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Scores for each fold: [0.74735605 0.73923445 0.76245655 0.78837209 0.74390244 0.77130045\n",
      " 0.74056604 0.76190476 0.75116279 0.74730539]\n",
      "F1 Score (Mean/Average): 0.755\n",
      "F1 Score (Standard Deviation): 0.015\n"
     ]
    }
   ],
   "source": [
    "# Import necessary library\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Perform 10-fold cross-validation on the training data\n",
    "f1_scores_list = cross_val_score(\n",
    "    LogisticRegression(),            # Model: Logistic Regression\n",
    "    X_train_tfidf,                   # Features: TF-IDF transformed training data\n",
    "    y_train,                         # Labels: Training labels\n",
    "    cv=10,                           # Number of folds\n",
    "    scoring=\"f1\"                     # Evaluation metric: F1 score\n",
    ")\n",
    "\n",
    "# Display the F1 scores for each fold\n",
    "print(f\"F1 Scores for each fold: {f1_scores_list}\")\n",
    "\n",
    "# Calculate and display the mean and standard deviation of the F1 scores\n",
    "print(\"F1 Score (Mean/Average): {:.3f}\".format(f1_scores_list.mean()))\n",
    "print(\"F1 Score (Standard Deviation): {:.3f}\".format(f1_scores_list.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **F1 Scores Across Folds:** The individual fold scores range from 0.739 to 0.788, demonstrating that the model performs reasonably consistently across different folds.\n",
    "- **Mean F1 Score:** The average F1 score of 0.755 is a reliable measure of the model's overall performance, indicating strong predictive capability for sentiment classification.\n",
    "- **Standard Deviation:** The low standard deviation of 0.015 indicates that the model performs consistently across different subsets of the data, confirming reliable generalization.\n",
    "- **Implications:** A high variation in F1 scores or a high standard deviation would suggest issues with data shuffling or an insufficient dataset size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Perform Hyperparameter Tuning\n",
    "\n",
    "Having validated our model's consistency through K-Fold Cross-Validation, the next step is to push its performance even further through hyperparameter tuning. While our Logistic Regression model achieved a solid mean F1 score of 0.755, the right combination of hyperparameters, such as the n-gram size and choice of classifier, can meaningfully improve accuracy and generalization. In this section, we'll systematically explore different parameter configurations to identify the optimal setup for our sentiment analysis task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Setting Up the Experiment\n",
    "\n",
    "To find the optimal model configuration, we need to systematically test every possible combination of our chosen hyperparameters:\n",
    "\n",
    "- the classifier type (LinearSVC vs. Logistic Regression) and\n",
    "- the n-gram size (1 to 4)\n",
    "\n",
    "and evaluate each using 10-fold cross-validation. The combination that produces the highest mean F1-score will be selected as our best configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LinearSVC, n-gram size: 1 => F1-score: 0.754\n",
      "Classifier: LinearSVC, n-gram size: 2 => F1-score: 0.766\n",
      "Classifier: LinearSVC, n-gram size: 3 => F1-score: 0.763\n",
      "Classifier: LinearSVC, n-gram size: 4 => F1-score: 0.763\n",
      "Classifier: LogisticRegression, n-gram size: 1 => F1-score: 0.756\n",
      "Classifier: LogisticRegression, n-gram size: 2 => F1-score: 0.754\n",
      "Classifier: LogisticRegression, n-gram size: 3 => F1-score: 0.748\n",
      "Classifier: LogisticRegression, n-gram size: 4 => F1-score: 0.745\n",
      "\n",
      "Best Configuration:\n",
      "Classifier: LinearSVC, Max n-gram size: 2, F1-score: 0.766\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize placeholders to store the best configuration\n",
    "best_score = -1.0\n",
    "best_classifier = None\n",
    "best_ngram_size = -1\n",
    "\n",
    "# Define the hyperparameters to test\n",
    "classifiers = [LinearSVC(), LogisticRegression(solver=\"sag\")]\n",
    "ngram_sizes = [1, 2, 3, 4]\n",
    "\n",
    "# Loop through all combinations of classifiers and n-gram sizes\n",
    "for classifier in classifiers:\n",
    "    for n in ngram_sizes:\n",
    "        # Define the vectorizer with the current n-gram size\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1, n))\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train)  # Transform training data\n",
    "\n",
    "        # Perform 10-fold cross-validation\n",
    "        f1_scores = cross_val_score(classifier, X_train_tfidf, y_train, cv=10, scoring='f1')\n",
    "        avg_f1_score = f1_scores.mean()  # Calculate average F1-score\n",
    "\n",
    "        # Print the result for this combination\n",
    "        print(f\"Classifier: {type(classifier).__name__}, n-gram size: {n} => F1-score: {avg_f1_score:.3f}\")\n",
    "\n",
    "        # Save the best configuration\n",
    "        if avg_f1_score > best_score:\n",
    "            best_score = avg_f1_score\n",
    "            best_classifier = classifier\n",
    "            best_ngram_size = n\n",
    "\n",
    "# Print the best configuration\n",
    "print(\"\\nBest Configuration:\")\n",
    "print(f\"Classifier: {type(best_classifier).__name__}, Max n-gram size: {best_ngram_size}, F1-score: {best_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all combinations tested, the results clearly show that LinearSVC with an n-gram size of 2 delivered the best performance, achieving an F1-score of 0.766. Now that we've identified the optimal hyperparameters, the next step is to retrain the model using this configuration on the full training dataset and evaluate it against the unseen test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training the Best Model\n",
    "\n",
    "After identifying the best combination of parameters, we can train the final model on the entire training dataset and evaluate it using the unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Model Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.76      0.75      1052\n",
      "           1       0.76      0.75      0.76      1081\n",
      "\n",
      "    accuracy                           0.75      2133\n",
      "   macro avg       0.75      0.75      0.75      2133\n",
      "weighted avg       0.76      0.75      0.75      2133\n",
      "\n",
      "Accuracy: 0.755\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Use the best configuration to train the final model\n",
    "final_vectorizer = TfidfVectorizer(ngram_range=(1, best_ngram_size))\n",
    "X_train_tfidf = final_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = final_vectorizer.transform(X_test)\n",
    "\n",
    "best_classifier.fit(X_train_tfidf, y_train)\n",
    "y_pred = best_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate and display results\n",
    "print(\"\\nFinal Model Results:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the final model using the optimal configuration:\n",
    "\n",
    "- classifier type: LinearSVC\n",
    "- n-gram range of (1, 2)\n",
    "\n",
    "yielded an accuracy of 75.5% and a consistent F1-score of 0.75â€“0.76 across both sentiment classes. This is a marginal improvement over the initial Logistic Regression baseline (F1 of 0.75), suggesting that for this dataset and preprocessing pipeline, the choice of classifier and n-gram range has limited impact. The balanced precision and recall scores for both classes confirm that the model is not biased toward either positive or negative sentiment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-movie-sentiment (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
