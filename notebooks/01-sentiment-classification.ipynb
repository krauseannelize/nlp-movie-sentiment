{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification of Movie Reviews\n",
    "\n",
    "This notebook builds a sentiment classifier to predict positive or negative sentiment from Rotten Tomatoes movie reviews using the Sentence Polarity dataset. The workflow covers data loading, text preprocessing, TF-IDF feature extraction, model training with hyperparameter tuning, and evaluation with K-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/annie/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/annie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/annie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/annie/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np  # for numerical operations\n",
    "import pandas as pd  # for data manipulation\n",
    "import random  # for shuffling the data\n",
    "import nltk\n",
    "import re  # for handling regular expressions\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer  # for lemmatizing words\n",
    "from nltk.corpus import stopwords  # for stop word removal\n",
    "from nltk.tokenize import word_tokenize  # for tokenizing sentences into words\n",
    "nltk.download('punkt_tab')  # Downloads the 'punkt' tokenizer table used for tokenization of text into sentences or words\n",
    "\n",
    "# Downloading necessary NLTK resources\n",
    "nltk.download('stopwords')  # List of common stop words in English\n",
    "nltk.download('punkt')  # Pre-trained tokenizer models\n",
    "nltk.download('wordnet')  # WordNet lemmatizer dataset\n",
    "\n",
    "# Libraries for text feature extraction and model training\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Convert text into numerical features (TF-IDF)\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic regression for classification\n",
    "from sklearn.svm import LinearSVC  # Support Vector Machines for classification\n",
    "\n",
    "# Libraries for model evaluation\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix  # For model evaluation metrics\n",
    "from sklearn.model_selection import KFold, cross_val_score  # For cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Read Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0  the rock is destined to be the 21st century's ...\n",
      "1  the gorgeously elaborate continuation of \" the...\n",
      "2                     effective but too-tepid biopic\n",
      "3  if you sometimes like to go to the movies to h...\n",
      "4  emerges as something rare , an issue movie tha...\n"
     ]
    }
   ],
   "source": [
    "# Read the negative and positive sentiment files\n",
    "df_sent_neg = pd.read_csv('../data/rt-polarity.neg', sep='\\t', header=None)  # Negative sentiment sentences\n",
    "df_sent_pos = pd.read_csv('../data/rt-polarity.pos', sep='\\t', header=None)  # Positive sentiment sentences\n",
    "\n",
    "# Display the first few rows of the positive dataset to understand its structure\n",
    "print(df_sent_pos.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column to 'sentence' for clarity\n",
    "df_sent_neg.rename(columns={0: \"sentence\"}, inplace=True)\n",
    "df_sent_pos.rename(columns={0: \"sentence\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing function\n",
    "def preprocess_text(sentences):\n",
    "    # Convert all tokens to lowercase\n",
    "    sentences = [sentence.lower() for sentence in sentences]\n",
    "\n",
    "    # Remove punctuation using regex\n",
    "    sentences = [re.sub(r\"[^\\w\\s]\", \"\", sentence) for sentence in sentences]\n",
    "\n",
    "    # Remove extra whitespace between words\n",
    "    sentences = [\" \".join(sentence.split()) for sentence in sentences]\n",
    "\n",
    "    # Tokenize sentences into words\n",
    "    sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))  # Load English stop words\n",
    "    filtered_sentences = []\n",
    "    for sentence in sentences:\n",
    "        filtered_sentence = [word for word in sentence if word not in stop_words]\n",
    "        filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "    # Lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentences = []\n",
    "    for sentence in filtered_sentences:\n",
    "        lemmatized_sentence = [lemmatizer.lemmatize(word) for word in sentence]\n",
    "        lemmatized_sentences.append(lemmatized_sentence)\n",
    "\n",
    "    return [' '.join(sentence) for sentence in lemmatized_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Apply Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simplistic silly tedious\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to both negative and positive sentences\n",
    "# Preprocess the sentences\n",
    "neg_preprocessed_sentences = preprocess_text(df_sent_neg['sentence'])\n",
    "pos_preprocessed_sentences = preprocess_text(df_sent_pos['sentence'])\n",
    "\n",
    "# Print the first preprocessed negative sentence\n",
    "print(neg_preprocessed_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Combine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine preprocessed negative and positive sentences\n",
    "sentences = neg_preprocessed_sentences + pos_preprocessed_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Create Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list for all labels\n",
    "polarities = []\n",
    "polarities.extend([0] * len(df_sent_neg))  # Label negative sentences as 0\n",
    "polarities.extend([1] * len(df_sent_pos))  # Label positive sentences as 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Shuffle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine sentences and labels into a single list\n",
    "combined = list(zip(sentences, polarities))\n",
    "\n",
    "# Shuffle the combined list\n",
    "random.shuffle(combined)\n",
    "\n",
    "# Split the shuffled list back into sentences and labels\n",
    "sentences[:], polarities[:] = zip(*combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 8529\n",
      "Size of test set: 2133\n"
     ]
    }
   ],
   "source": [
    "# Define train-test split ratio\n",
    "train_test_ratio = 0.8\n",
    "\n",
    "# Calculate the size of the training set\n",
    "train_set_size = int(train_test_ratio * len(sentences))\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test = sentences[:train_set_size], sentences[train_set_size:]\n",
    "y_train, y_test = polarities[:train_set_size], polarities[train_set_size:]\n",
    "\n",
    "# Print sizes of training and test sets\n",
    "print(\"Size of training set:\", len(X_train))\n",
    "print(\"Size of test set:\", len(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-movie-sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
